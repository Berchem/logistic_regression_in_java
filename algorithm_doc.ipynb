{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation in probibilistic model\n",
    "\n",
    "Assume data generate via a probabilistic model :\n",
    "\\begin{align}\n",
    "d \\sim P(d|\\theta)\n",
    "\\end{align}\n",
    "\n",
    "$P(d|\\theta)$ : Probability distribution underlying the data\n",
    "* $\\theta$ : fixed, but unknown distribution parameter\n",
    "\n",
    "Given $n$ independent and identically distributed samples of data $D$ :\n",
    "\\begin{align}\n",
    "D = \\{d_1, d_2, d_3, ..., d_n\\}\n",
    "\\end{align}\n",
    "\n",
    "To estimate parameter $\\theta$ that optimum description to the data, we could use the maximum-a-posteriori estimation method.\n",
    "<p><br><br></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maximum-a-posteriori estimation\n",
    "\n",
    "**Maximum-a-Posteriori** (**MAP**) is to choose $\\theta$ which maximizes the posterior probability of $\\theta$.\n",
    "\n",
    "**Posterior probability of $\\theta$** is given by the Bayes Rule :\n",
    "\\begin{align}\n",
    "P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}\n",
    "\\end{align}\n",
    "\n",
    "where, \n",
    "> $p(D|\\theta)$ : **likelihood function**\n",
    ">\n",
    "> $p(\\theta)$ : **prior probapility of $\\theta$**, without having seen any data\n",
    ">\n",
    "> $p(D)$ : **probapility of data**, independent of $\\theta$\n",
    "\n",
    "**Posterior probability of $\\theta$** could be driven as :\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta}_{MAP} = {\\arg\\max}_{\\theta}P(\\theta|D) &= {\\arg\\max}_{\\theta}\\frac{P(D|\\theta)P(\\theta)}{P(D)}\\\\\n",
    "\\text{probapility of data is constant  }\\Rightarrow&= {\\arg\\max}_{\\theta}P(D|\\theta)P(\\theta)\\\\\n",
    "\\text{the natural logarithm is a strictly increasing function  }\\Rightarrow&= {\\arg\\max}_{\\theta}log(P(D|\\theta)P(\\theta))\\\\\n",
    "\\text{the strictly increasing function is, if }x_1 < x_2,\\text{ then }f(x_1) < f(x_2)\\Rightarrow&= {\\arg\\max}_{\\theta}[logP(D|\\theta) + logP(\\theta)]\\\\\n",
    "\\hat{\\theta}_{MAP} &= {\\arg\\max}_{\\theta}[{\\sum}logP(d_i|\\theta) + logP(\\theta)]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find the maxima of posteriori probability function\n",
    "\n",
    "Given $f(x)$, we **differentiate once** to find $f'(x)$. \n",
    "\n",
    "Set $f'(x) = 0$ and solve for $x$. Using our above observation, the x values we find are the $x$-coordinates of our maxima and minima.\n",
    "\n",
    "Substitute these $x$-values back into $f(x)$. This gives the corresponding $y$-coordinates of our maxima or minima.\n",
    "\n",
    "Since, to find the maxima of posterior probability function is equivalent **to find the root of the gradient of posteriori probability function**.\n",
    "\n",
    "Then use of the gradient of the objective and Hessian matrix to converge in Newton's iterative method to find the maxima of posterior probability function.\n",
    "<p><br><br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "Newton's method is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function.\n",
    "\n",
    "\\begin{align}\n",
    "x:f(x)=0\n",
    "\\end{align}\n",
    "\n",
    "The process is repeated as :\n",
    "\n",
    "\\begin{align}\n",
    "x_{new} = x_{old} - \\frac{f(x_{old})}{f'(x_{old})}\n",
    "\\end{align}\n",
    "\n",
    "while $x_{new}$ close enough to $x_{old}$, &nbsp;$x_{new}{\\approx}x_{old}$, &nbsp;the sequence converged, &nbsp;$x_{new}$ is approximate to the root $x$ of the function $f(x)$.\n",
    "\n",
    "In programing, we could code that as:\n",
    "```python\n",
    "if abs(w_new - w_old) < tolerance  # given a tolerance = 1e-8\n",
    "    break\n",
    "```\n",
    "\n",
    ">known a curve :\n",
    ">\n",
    ">\\begin{align}\n",
    "y = f(x)\n",
    "\\end{align}\n",
    ">\n",
    ">a point at $x_n$ had a $y_n$ :\n",
    ">\n",
    ">\\begin{align}\n",
    "y_n = f(x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">and another point $x$ approximate to $x_n$ had a $y$ :\n",
    ">\n",
    ">\\begin{align}\n",
    "y = f(x_n) + \\frac{f(x) - f(x_n)}{(x - x_n)}(x - x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">the $\\frac{f(x) - f(x_n)}{(x - x_n)}$ is the tangent line at $x_n$\n",
    ">\n",
    ">\\begin{align}\n",
    "y = f(x_n) + f'(x_n)(x - x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">where $f'$ denotes the derivative of the function $f$\n",
    ">\n",
    ">the root, the value of $x$ such that $y = 0$, is then used as the next approximation to the root, $x_{n+1}$ : \n",
    ">\n",
    ">\\begin{align}\n",
    "0 = f(x_n) + f'(x_n)(x_{n+1} - x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">solving for $x_{n+1}$ gives\n",
    ">\\begin{align}\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "\\end{align}\n",
    "<p><br><br></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The logistic regression model is :\n",
    "\n",
    "\\begin{align}\n",
    "p(y=\\pm1|\\mathbf{x}, \\mathbf{w}) & = \\frac{1}{1+e^{-y\\mathbf{w}^T\\mathbf{x}}} \n",
    "\\end{align}\n",
    "\n",
    "A common prior to use with MAP is :\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{w}) & \\sim \\mathcal{N}(0, \\lambda^{-1}\\mathbf{I}) \n",
    "\\end{align}\n",
    "\n",
    "where $\\lambda$ is regularization strength\n",
    "\n",
    "Given a data set $(\\mathbf{X}, \\mathbf{y}) = [(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), ..., (\\mathbf{x}_n, y_n)]$, we want to find the parameter vector $\\mathbf{w}$ which maximizes :\n",
    "\n",
    "\\begin{align}\n",
    "l(\\mathbf{w}) = -\\sum^{n}_{i=1}log(1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}) - \\frac{\\lambda}{2}{\\mathbf{w}^T\\mathbf{w}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    ">Driven :\n",
    ">\n",
    ">data set $(\\mathbf{X}, \\mathbf{y}) = [(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), ..., (\\mathbf{x}_n, y_n)]$ could be seen as $D = \\{d_1, d_2, d_3, ..., d_n\\}$\n",
    ">\n",
    ">the parameter vector $\\mathbf{w}$ was $\\theta$\n",
    ">\n",
    ">so the maximum posterior probability of the parameter vector $\\mathbf{w}$ could be written as : \n",
    ">\n",
    ">\\begin{align}\n",
    "\\mathbf{w}_{MAP} &= {\\arg\\max}_{\\theta}[\\sum^{n}_{i=1}logP(y_i, \\mathbf{x}_{i}|\\mathbf{w}) + logP(\\mathbf{w})]\n",
    "\\end{align}\n",
    ">\n",
    ">\\begin{align}\n",
    "{{\\mathbf{w}}}_{MAP} &= {\\arg\\max}_{\\mathbf{w}}\\sum^{n}_{i=1}log(\\frac{1}{1+e^{-y_i{{\\mathbf{w}}}^T{{\\mathbf{x}}}_i}}) + log(\\frac{1}{\\sqrt{\\frac{2\\pi}{\\lambda}}}e^{-\\frac{\\lambda {\\mathbf{w}}^T{\\mathbf{w}}}{2}})\\\\\n",
    "&= {\\arg\\max}_{\\mathbf{w}}\\sum^{n}_{i=1}log({1+e^{-y_i{\\mathbf{w}}^T{\\mathbf{x}}_i}})^{-1} + [log(\\frac{1}{\\sqrt{\\frac{2\\pi}{\\lambda}}}) + log(e^{-\\frac{\\lambda \\mathbf{w}^T{\\mathbf{w}}}{2}})]\\\\\n",
    "&= {\\arg\\max}_{\\mathbf{w}}-\\sum^{n}_{i=1}log({1+e^{-y_i{\\mathbf{w}}^T{\\mathbf{x}}_i}}) - \\frac{\\lambda}{2}{\\mathbf{w}}^T{\\mathbf{w}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The posterior probability function : \n",
    "\n",
    "\\begin{align}\n",
    "l(\\mathbf{w}) = -{\\sum}^{n}_{i=1}log(1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}) - \\frac{\\lambda}{2}{\\mathbf{w}}^T{{\\mathbf{w}}}\n",
    "\\end{align}\n",
    "\n",
    "The gradient of the objective, **first-order partial derivatives** of the posterior function :\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{g} = \\triangledown_{\\mathbf{w}}l(\\mathbf{w}) = \\sum^{n}_{i=1}(1-\\frac{1}{1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}})y_i\\mathbf{x}_i - \\lambda\\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "The Hessian matrix, a square matrix of **second-order partial derivatives** of the posterior function :\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{H} = \\frac{d^2l({\\mathbf{w}})}{d{\\mathbf{w}}d{\\mathbf{w}}^T} = -\\sum^{n}_{i=1}(\\frac{1}{1+e^{-{\\mathbf{w}}^T{\\mathbf{x}}_i}})(1-\\frac{1}{1+e^{-{\\mathbf{w}}^T{\\mathbf{x}}_i}}){\\mathbf{x}}_i{\\mathbf{x}}^T_i - \\lambda\\mathbf{I}\n",
    "\\end{align}\n",
    "\n",
    "which in matrix form can be written :\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{H} = -\\mathbf{XAX}^{T} - \\lambda\\mathbf{I}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{A}$ is a diagonal matrix :\n",
    "\\begin{align}\n",
    "a_{ii} = \\frac{1}{1+e^{{\\mathbf{w}}^T{\\mathbf{x}}_i}}(1-\\frac{1}{1+e^{{\\mathbf{w}}^T{\\mathbf{x}}_i}})\n",
    "\\end{align}\n",
    "<p><br><br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have known all of the algorithm details now, let's try organizing the equations to code.\n",
    "<br><br>\n",
    "```python\n",
    "# if we had defined the variable x, y, w_old, w_new, lamb, tol, lim, i\n",
    "# x.shape = (m, n), <type 'numpy.ndarray'>\n",
    "# y.shape = (m, 1), <type 'numpy.ndarray'>\n",
    "# w_old.shape = (n, 1), <type 'numpy.'>\n",
    "# w_new.shape = (n, 1), <type 'numpy.ndarray'>\n",
    "# lamb.shape = (n, n), <type 'numpy.ndarray'>, \n",
    "#     take the regularization parameter lambda's name as lamb, \n",
    "#     because lambda is the keyword for anonymous function in python.\n",
    "# tol, <type 'float'>, the tolerance for difference between parameter i + 1 and i\n",
    "# lim, <type 'int'>, maxima iterations\n",
    "# i, <type 'int'>, counts of iterations\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def hessian(X, A, lamb):\n",
    "    return -X.T.dot(A).dot(X) + lamb\n",
    "\n",
    "\n",
    "while i < iter_lim:\n",
    "    # update parameter estimation\n",
    "    w_old = w_new\n",
    "    \n",
    "    # calculate diagonal matrix, that a is the A_ii\n",
    "    a = sigmoid_prime(x.dot(w_old))\n",
    "    A = np.diagflat(a)\n",
    "    \n",
    "    # Hessian matrix\n",
    "    H = hessian(x, A, lamb)\n",
    "    \n",
    "    # a part of gradient objective and combine previous iterated parameter\n",
    "    z = x.dot(w_old) + (1 - sigmoid(y * x.dot(w_old))) * y / a\n",
    "    XAz = x.T.dot(A).dot(z)\n",
    "    \n",
    "    # calculate current parameter\n",
    "    w_new = np.linalg.inv(-H).dot(XAz)\n",
    "    \n",
    "    # calculate tolerance and iteration time to break loop\n",
    "    if all(abs(w - w_old) < tol)::\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        i += 1\n",
    "```\n",
    "<p><br><br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* Minka, T. P. (2003). A comparison of numerical optimizers for logistic regression. Unpublished draft.\n",
    "* Grus, J. (2015). Data science from scratch: first principles with python (pp.270-279). \" O'Reilly Media, Inc.\".\n",
    "* The Pennsylvania State University, Department of Statistics Online Programs. (n.d.) Retrieved June 29, 2018 from [online course](https://onlinecourses.science.psu.edu/stat414/node/191/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
